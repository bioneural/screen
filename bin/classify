#!/usr/bin/env ruby
# bin/classify — classify input against a condition via LLM
# MIT License — Copyright (c) 2026 Kerry Ivan Kurian
#
# Reads JSON from stdin:
#   { "condition": "...", "model": "gemma3:1b",
#     "file_path": "...", "command": "...", "prompt": "...", "content": "..." }
#
# Prints "yes" or "no" to stdout. Exits 0 on success, 1 on failure.
# With --score flag, prints the numeric confidence score (0.0-1.0) instead.
#
# Dependencies: ruby stdlib, ollama

require_relative '../lib/classifier'
require 'json'
require 'open3'

score_mode = ARGV.delete('--score')

SPILL_HOME = ENV['SPILL_HOME'] || File.expand_path('../../spill', __dir__)
if File.directory?(SPILL_HOME)
  require File.join(SPILL_HOME, 'lib', 'spill')
  Spill.configure(tool: 'classify')
end

if ARGV.first == 'doctor'
  report = {}

  report['ruby'] = { 'version' => RUBY_VERSION, 'ok' => true }

  # Classifier library
  report['classifier_lib'] = { 'ok' => defined?(Classifier) || defined?(CLASSIFIER_MODEL) ? true : true }

  # Ollama
  ollama_out, ollama_status = Open3.capture2('ollama', '--version')
  report['ollama'] = {
    'version' => ollama_status.success? ? ollama_out.strip : nil,
    'ok' => ollama_status.success?
  }

  # Ollama reachable
  begin
    require 'net/http'
    require 'uri'
    host = ENV.fetch('OLLAMA_HOST', 'http://localhost:11434')
    uri = URI("#{host}/api/tags")
    response = Net::HTTP.get_response(uri)
    report['ollama_api'] = { 'host' => host, 'ok' => response.is_a?(Net::HTTPSuccess) }
  rescue => e
    report['ollama_api'] = { 'host' => host, 'ok' => false, 'error' => e.message }
  end

  # Spill loadable
  report['spill'] = { 'path' => SPILL_HOME, 'ok' => defined?(Spill) ? true : false }

  report['ok'] = report.values.all? { |v| v.is_a?(Hash) ? v['ok'] != false : true }
  puts JSON.pretty_generate(report)
  exit(report['ok'] ? 0 : 1)
end

input = JSON.parse($stdin.read)
condition = input.delete('condition')
model = input.delete('model') || CLASSIFIER_MODEL

fields = build_classifier_input(input)
if fields.empty?
  defined?(Spill) ? Spill.error("no input fields provided") : $stderr.puts("classify: no input fields provided")
  exit 1
end

prompt = CLASSIFIER_TEMPLATE % { condition: condition, fields: fields }

# Try logprob-based classification first
result = classify_with_logprobs(prompt, model)

if result
  if score_mode
    puts format('%.3f', result[:score])
  else
    puts result[:answer]
  end
else
  # Fallback: shell out to ollama run (no score available)
  if score_mode
    msg = "logprob classification failed; --score unavailable in fallback mode"
    defined?(Spill) ? Spill.warn(msg) : $stderr.puts("classify: #{msg}")
  end

  stdout, status = Open3.capture2('ollama', 'run', model, stdin_data: prompt, err: File::NULL)

  unless status.success?
    defined?(Spill) ? Spill.error("ollama exited with #{status.exitstatus}") : $stderr.puts("classify: ollama exited with #{status.exitstatus}")
    exit 1
  end

  answer = stdout.strip.downcase.start_with?('yes') ? 'yes' : 'no'
  if score_mode
    # Best-effort: emit 1.0 or 0.0 based on text answer
    puts answer == 'yes' ? '1.000' : '0.000'
  else
    puts answer
  end
end
